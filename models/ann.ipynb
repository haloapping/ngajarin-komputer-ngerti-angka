{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b59c2b5-cf4c-45f8-a6a7-524d68324207",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49da77f8-5345-4d52-b9c7-4981d15e2d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "from datetime import datetime\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d117c-bf7a-4da0-bebc-4e95df1ba756",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37d7131-05dc-491e-9e77-5fc7dca3604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: (x / 255) - 0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9edb694d-a778-477b-ab67-62d63a3d2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"../datasets/train.csv\")\n",
    "test_dataset = pd.read_csv(\"../datasets/test.csv\")\n",
    "\n",
    "X_train = transform(torch.FloatTensor(train_dataset.loc[:, train_dataset.columns != 'label'].to_numpy()))\n",
    "y_train = torch.LongTensor(train_dataset.label.to_numpy())\n",
    "\n",
    "X_test = transform(torch.FloatTensor(test_dataset.loc[:, test_dataset.columns != 'label'].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24e7b56-e955-4e63-8feb-f167419e0bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([42000, 784]), torch.Size([42000]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c481f5-deb6-4c81-8f0c-2b8b714438e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28000, 784])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57016bf3-0c57-4254-aea4-dcf20f810ba7",
   "metadata": {},
   "source": [
    "# Tensor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa12b80f-347c-4bb9-82b4-b0e03a32967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90ebfa-1b99-45f8-b2f6-5f4a44c1ddca",
   "metadata": {},
   "source": [
    "# Dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89617e3b-e478-4c76-938b-fabd414b5d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42191d70-bf55-4093-b89a-d2000ceadb17",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d83d4476-bbc3-470d-a36e-d17f3ab18264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (feature): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.6, inplace=False)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Linear(in_features=2048, out_features=10, bias=True)\n",
       "  )\n",
       "  (output): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, width_img, height_img):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(in_features=width_img * height_img, out_features=1024),\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=10)\n",
    "        )\n",
    "        self.output = nn.Softmax(dim=1)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x_feature = self.feature(x)\n",
    "        x_output = self.output(x_feature)\n",
    "        \n",
    "        return x_output\n",
    "    \n",
    "    def get_feature(self, x):        \n",
    "        return self.feature[0:4](x)\n",
    "\n",
    "def init_wb_kaiming_uniform(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(layer.weight.data, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n",
    "        nn.init.kaiming_uniform_(layer.bias.data.reshape(layer.bias.data.shape[0], 1), a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\")    \n",
    "\n",
    "model = CustomModel(28, 28).to(device)\n",
    "model.apply(init_wb_kaiming_uniform)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36100060-b23a-485c-bc77-303bf510bc28",
   "metadata": {},
   "source": [
    "# Eval Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae54800-8238-42bc-834c-60c738d4e394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Accuracy(), Accuracy())"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metric_train = Accuracy().to(device)\n",
    "eval_metric_test = Accuracy().to(device)\n",
    "eval_metric_train, eval_metric_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416b728-12f9-40ed-aa5d-304b628cb6aa",
   "metadata": {},
   "source": [
    "# Loss Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f737e9ec-3951-4b0a-8ac8-08746f69229a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_metric = nn.CrossEntropyLoss().to(device)\n",
    "loss_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fdd1ef-5d19-456b-9986-7e2bcc2c5b6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e84086b9-f4e6-4227-b849-f98a612e9b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 6.25e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=6.25e-05)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9fda2f-19a5-49a6-9672-acc2cc95fafa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d6f0a29-738f-4d54-8e8d-28b9eff105ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, loss_metric, eval_metric, optimizer, show_metric_every):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, (data, actual_labels) in enumerate(train_dataloader, 1):\n",
    "        data = data.to(device)\n",
    "        actual_labels = actual_labels.to(device)\n",
    "        \n",
    "        # Forward Propagation\n",
    "        pred_labels = model(data)\n",
    "        loss = loss_metric(pred_labels, actual_labels)\n",
    "        evaluate = eval_metric(pred_labels, actual_labels)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if batch % show_metric_every == 0 or batch == len(train_dataloader):\n",
    "            print(f\"Batch {batch} >> loss: {loss:.3f} | acc: {evaluate:.3f}\")\n",
    "        \n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    mean_loss = torch.mean(torch.tensor(losses))\n",
    "    mean_metric = eval_metric.compute()\n",
    "    \n",
    "    return mean_loss, mean_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85c4d2-a25b-4aa5-bb75-a672ad0a0169",
   "metadata": {},
   "source": [
    "# Testing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cba1ad4a-3ab6-4a80-b805-96c3a479876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_step(dataloader, model, loss_metric, eval_metric, optimizer, show_metric_every):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch, (data, actual_labels) in enumerate(test_dataloader, 1):\n",
    "        data = data.to(device)\n",
    "        actual_labels = actual_labels.to(device)\n",
    "        \n",
    "        # Forward Propagation\n",
    "        pred_labels = model(data)\n",
    "        loss = loss_metric(pred_labels, actual_labels)\n",
    "        evaluate = eval_metric(pred_labels, actual_labels)\n",
    "        losses.append(loss_metric)\n",
    "        \n",
    "        if batch % show_metric_every == 0 or batch == len(test_dataloader):\n",
    "            print(f\"Batch {batch} >> loss: {loss:.3f} | acc: {evaluate:.3f}\")\n",
    "\n",
    "    mean_loss = torch.mean(torch.tensor(losses))\n",
    "    mean_metric = eval_metric.compute()\n",
    "    \n",
    "    return mean_loss, mean_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3e00f-d166-415b-8a32-aa61af2245c1",
   "metadata": {},
   "source": [
    "# Fitting Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcda9b06-b35b-4f5c-bbc0-b43bf02b56a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 2.291 | acc: 0.156\n",
      "Batch 40 >> loss: 2.271 | acc: 0.188\n",
      "Batch 60 >> loss: 2.180 | acc: 0.266\n",
      "Batch 80 >> loss: 2.070 | acc: 0.438\n",
      "Batch 100 >> loss: 2.105 | acc: 0.375\n",
      "Batch 120 >> loss: 2.033 | acc: 0.453\n",
      "Batch 140 >> loss: 1.963 | acc: 0.547\n",
      "Batch 160 >> loss: 1.914 | acc: 0.562\n",
      "Batch 180 >> loss: 1.914 | acc: 0.531\n",
      "Batch 200 >> loss: 1.890 | acc: 0.656\n",
      "Batch 220 >> loss: 1.931 | acc: 0.547\n",
      "Batch 240 >> loss: 1.835 | acc: 0.656\n",
      "Batch 260 >> loss: 1.954 | acc: 0.500\n",
      "Batch 280 >> loss: 1.826 | acc: 0.641\n",
      "Batch 300 >> loss: 1.799 | acc: 0.672\n",
      "Batch 320 >> loss: 1.716 | acc: 0.797\n",
      "Batch 340 >> loss: 1.696 | acc: 0.781\n",
      "Batch 360 >> loss: 1.754 | acc: 0.750\n",
      "Batch 380 >> loss: 1.815 | acc: 0.641\n",
      "Batch 400 >> loss: 1.798 | acc: 0.672\n",
      "Batch 420 >> loss: 1.762 | acc: 0.688\n",
      "Batch 440 >> loss: 1.710 | acc: 0.766\n",
      "Batch 460 >> loss: 1.803 | acc: 0.688\n",
      "Batch 480 >> loss: 1.750 | acc: 0.703\n",
      "Batch 500 >> loss: 1.739 | acc: 0.781\n",
      "Batch 520 >> loss: 1.673 | acc: 0.781\n",
      "Batch 540 >> loss: 1.728 | acc: 0.750\n",
      "Batch 560 >> loss: 1.751 | acc: 0.734\n",
      "Batch 580 >> loss: 1.758 | acc: 0.719\n",
      "Batch 600 >> loss: 1.693 | acc: 0.781\n",
      "Batch 620 >> loss: 1.707 | acc: 0.766\n",
      "Batch 640 >> loss: 1.776 | acc: 0.672\n",
      "Batch 657 >> loss: 1.535 | acc: 0.938\n",
      "Average train loss : 1.856\n",
      "Average train acc  : 0.622\n",
      "\n",
      "========================================\n",
      "\n",
      "EPOCH 2\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 1.737 | acc: 0.734\n",
      "Batch 40 >> loss: 1.671 | acc: 0.812\n",
      "Batch 60 >> loss: 1.601 | acc: 0.875\n",
      "Batch 80 >> loss: 1.757 | acc: 0.719\n",
      "Batch 100 >> loss: 1.736 | acc: 0.750\n",
      "Batch 120 >> loss: 1.680 | acc: 0.797\n",
      "Batch 140 >> loss: 1.623 | acc: 0.844\n",
      "Batch 160 >> loss: 1.643 | acc: 0.828\n",
      "Batch 180 >> loss: 1.688 | acc: 0.812\n",
      "Batch 200 >> loss: 1.728 | acc: 0.734\n",
      "Batch 220 >> loss: 1.650 | acc: 0.812\n",
      "Batch 240 >> loss: 1.681 | acc: 0.797\n",
      "Batch 260 >> loss: 1.699 | acc: 0.734\n",
      "Batch 280 >> loss: 1.557 | acc: 0.938\n",
      "Batch 300 >> loss: 1.610 | acc: 0.875\n",
      "Batch 320 >> loss: 1.636 | acc: 0.828\n",
      "Batch 340 >> loss: 1.669 | acc: 0.781\n",
      "Batch 360 >> loss: 1.604 | acc: 0.875\n",
      "Batch 380 >> loss: 1.664 | acc: 0.797\n",
      "Batch 400 >> loss: 1.693 | acc: 0.781\n",
      "Batch 420 >> loss: 1.711 | acc: 0.766\n",
      "Batch 440 >> loss: 1.633 | acc: 0.844\n",
      "Batch 460 >> loss: 1.594 | acc: 0.891\n",
      "Batch 480 >> loss: 1.628 | acc: 0.844\n",
      "Batch 500 >> loss: 1.586 | acc: 0.875\n",
      "Batch 520 >> loss: 1.650 | acc: 0.844\n",
      "Batch 540 >> loss: 1.681 | acc: 0.781\n",
      "Batch 560 >> loss: 1.578 | acc: 0.906\n",
      "Batch 580 >> loss: 1.699 | acc: 0.766\n",
      "Batch 600 >> loss: 1.640 | acc: 0.828\n",
      "Batch 620 >> loss: 1.591 | acc: 0.891\n",
      "Batch 640 >> loss: 1.631 | acc: 0.844\n",
      "Batch 657 >> loss: 1.664 | acc: 0.812\n",
      "Average train loss : 1.666\n",
      "Average train acc  : 0.808\n",
      "\n",
      "Improve! 😄\n",
      "========================================\n",
      "\n",
      "EPOCH 3\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 1.641 | acc: 0.844\n",
      "Batch 40 >> loss: 1.644 | acc: 0.844\n",
      "Batch 60 >> loss: 1.705 | acc: 0.781\n",
      "Batch 80 >> loss: 1.587 | acc: 0.891\n",
      "Batch 100 >> loss: 1.583 | acc: 0.922\n",
      "Batch 120 >> loss: 1.590 | acc: 0.875\n",
      "Batch 140 >> loss: 1.641 | acc: 0.844\n",
      "Batch 160 >> loss: 1.565 | acc: 0.906\n",
      "Batch 180 >> loss: 1.569 | acc: 0.922\n",
      "Batch 200 >> loss: 1.570 | acc: 0.906\n",
      "Batch 220 >> loss: 1.601 | acc: 0.891\n",
      "Batch 240 >> loss: 1.615 | acc: 0.844\n",
      "Batch 260 >> loss: 1.503 | acc: 0.984\n",
      "Batch 280 >> loss: 1.552 | acc: 0.938\n",
      "Batch 300 >> loss: 1.548 | acc: 0.938\n",
      "Batch 320 >> loss: 1.600 | acc: 0.891\n",
      "Batch 340 >> loss: 1.594 | acc: 0.891\n",
      "Batch 360 >> loss: 1.616 | acc: 0.859\n",
      "Batch 380 >> loss: 1.644 | acc: 0.812\n",
      "Batch 400 >> loss: 1.567 | acc: 0.906\n",
      "Batch 420 >> loss: 1.535 | acc: 0.953\n",
      "Batch 440 >> loss: 1.598 | acc: 0.859\n",
      "Batch 460 >> loss: 1.580 | acc: 0.891\n",
      "Batch 480 >> loss: 1.560 | acc: 0.906\n",
      "Batch 500 >> loss: 1.570 | acc: 0.906\n",
      "Batch 520 >> loss: 1.586 | acc: 0.891\n",
      "Batch 540 >> loss: 1.585 | acc: 0.906\n",
      "Batch 560 >> loss: 1.570 | acc: 0.891\n",
      "Batch 580 >> loss: 1.551 | acc: 0.938\n",
      "Batch 600 >> loss: 1.594 | acc: 0.859\n",
      "Batch 620 >> loss: 1.625 | acc: 0.828\n",
      "Batch 640 >> loss: 1.545 | acc: 0.938\n",
      "Batch 657 >> loss: 1.531 | acc: 0.938\n",
      "Average train loss : 1.598\n",
      "Average train acc  : 0.877\n",
      "\n",
      "Improve! 😄\n",
      "========================================\n",
      "\n",
      "EPOCH 4\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 1.536 | acc: 0.953\n",
      "Batch 40 >> loss: 1.560 | acc: 0.922\n",
      "Batch 60 >> loss: 1.573 | acc: 0.891\n",
      "Batch 80 >> loss: 1.569 | acc: 0.891\n",
      "Batch 100 >> loss: 1.572 | acc: 0.891\n",
      "Batch 120 >> loss: 1.532 | acc: 0.969\n",
      "Batch 140 >> loss: 1.610 | acc: 0.844\n",
      "Batch 160 >> loss: 1.582 | acc: 0.906\n",
      "Batch 180 >> loss: 1.549 | acc: 0.953\n",
      "Batch 200 >> loss: 1.602 | acc: 0.859\n",
      "Batch 220 >> loss: 1.572 | acc: 0.891\n",
      "Batch 240 >> loss: 1.540 | acc: 0.922\n",
      "Batch 260 >> loss: 1.525 | acc: 0.953\n",
      "Batch 280 >> loss: 1.604 | acc: 0.859\n",
      "Batch 300 >> loss: 1.578 | acc: 0.906\n",
      "Batch 320 >> loss: 1.592 | acc: 0.844\n",
      "Batch 340 >> loss: 1.590 | acc: 0.859\n",
      "Batch 360 >> loss: 1.532 | acc: 0.953\n",
      "Batch 380 >> loss: 1.606 | acc: 0.859\n",
      "Batch 400 >> loss: 1.591 | acc: 0.891\n",
      "Batch 420 >> loss: 1.656 | acc: 0.844\n",
      "Batch 440 >> loss: 1.545 | acc: 0.938\n",
      "Batch 460 >> loss: 1.519 | acc: 0.953\n",
      "Batch 480 >> loss: 1.529 | acc: 0.938\n",
      "Batch 500 >> loss: 1.553 | acc: 0.922\n",
      "Batch 520 >> loss: 1.559 | acc: 0.906\n",
      "Batch 540 >> loss: 1.539 | acc: 0.938\n",
      "Batch 560 >> loss: 1.547 | acc: 0.906\n",
      "Batch 580 >> loss: 1.544 | acc: 0.938\n",
      "Batch 600 >> loss: 1.563 | acc: 0.906\n",
      "Batch 620 >> loss: 1.587 | acc: 0.891\n",
      "Batch 640 >> loss: 1.589 | acc: 0.875\n",
      "Batch 657 >> loss: 1.546 | acc: 0.938\n",
      "Average train loss : 1.571\n",
      "Average train acc  : 0.902\n",
      "\n",
      "Improve! 😄\n",
      "========================================\n",
      "\n",
      "EPOCH 5\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 1.543 | acc: 0.922\n",
      "Batch 40 >> loss: 1.608 | acc: 0.859\n",
      "Batch 60 >> loss: 1.528 | acc: 0.953\n",
      "Batch 80 >> loss: 1.553 | acc: 0.906\n",
      "Batch 100 >> loss: 1.546 | acc: 0.938\n",
      "Batch 120 >> loss: 1.630 | acc: 0.828\n",
      "Batch 140 >> loss: 1.499 | acc: 0.969\n",
      "Batch 160 >> loss: 1.517 | acc: 0.953\n",
      "Batch 180 >> loss: 1.624 | acc: 0.844\n",
      "Batch 200 >> loss: 1.589 | acc: 0.875\n",
      "Batch 220 >> loss: 1.563 | acc: 0.906\n",
      "Batch 240 >> loss: 1.516 | acc: 0.969\n",
      "Batch 260 >> loss: 1.576 | acc: 0.891\n",
      "Batch 280 >> loss: 1.547 | acc: 0.922\n",
      "Batch 300 >> loss: 1.594 | acc: 0.859\n",
      "Batch 320 >> loss: 1.538 | acc: 0.922\n",
      "Batch 340 >> loss: 1.497 | acc: 0.969\n",
      "Batch 360 >> loss: 1.519 | acc: 0.922\n",
      "Batch 380 >> loss: 1.517 | acc: 0.953\n",
      "Batch 400 >> loss: 1.523 | acc: 0.953\n",
      "Batch 420 >> loss: 1.591 | acc: 0.875\n",
      "Batch 440 >> loss: 1.586 | acc: 0.875\n",
      "Batch 460 >> loss: 1.590 | acc: 0.875\n",
      "Batch 480 >> loss: 1.504 | acc: 0.969\n",
      "Batch 500 >> loss: 1.593 | acc: 0.844\n",
      "Batch 520 >> loss: 1.505 | acc: 0.969\n",
      "Batch 540 >> loss: 1.565 | acc: 0.906\n",
      "Batch 560 >> loss: 1.515 | acc: 0.969\n",
      "Batch 580 >> loss: 1.541 | acc: 0.922\n",
      "Batch 600 >> loss: 1.586 | acc: 0.875\n",
      "Batch 620 >> loss: 1.585 | acc: 0.875\n",
      "Batch 640 >> loss: 1.506 | acc: 0.969\n",
      "Batch 657 >> loss: 1.533 | acc: 0.938\n",
      "Average train loss : 1.558\n",
      "Average train acc  : 0.911\n",
      "\n",
      "Improve! 😄\n",
      "========================================\n",
      "\n",
      "EPOCH 6\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 1.569 | acc: 0.875\n",
      "Batch 40 >> loss: 1.542 | acc: 0.922\n",
      "Batch 60 >> loss: 1.543 | acc: 0.938\n",
      "Batch 80 >> loss: 1.564 | acc: 0.891\n",
      "Batch 100 >> loss: 1.535 | acc: 0.953\n",
      "Batch 120 >> loss: 1.563 | acc: 0.922\n",
      "Batch 140 >> loss: 1.590 | acc: 0.859\n",
      "Batch 160 >> loss: 1.607 | acc: 0.859\n",
      "Batch 180 >> loss: 1.505 | acc: 0.969\n",
      "Batch 200 >> loss: 1.544 | acc: 0.953\n",
      "Batch 220 >> loss: 1.517 | acc: 0.953\n",
      "Batch 240 >> loss: 1.541 | acc: 0.922\n",
      "Batch 260 >> loss: 1.564 | acc: 0.906\n",
      "Batch 280 >> loss: 1.580 | acc: 0.891\n",
      "Batch 300 >> loss: 1.559 | acc: 0.906\n",
      "Batch 320 >> loss: 1.521 | acc: 0.953\n",
      "Batch 340 >> loss: 1.539 | acc: 0.938\n",
      "Batch 360 >> loss: 1.576 | acc: 0.891\n",
      "Batch 380 >> loss: 1.550 | acc: 0.938\n",
      "Batch 400 >> loss: 1.567 | acc: 0.906\n",
      "Batch 420 >> loss: 1.562 | acc: 0.906\n",
      "Batch 440 >> loss: 1.555 | acc: 0.906\n",
      "Batch 460 >> loss: 1.579 | acc: 0.906\n",
      "Batch 480 >> loss: 1.544 | acc: 0.906\n",
      "Batch 500 >> loss: 1.557 | acc: 0.906\n",
      "Batch 520 >> loss: 1.540 | acc: 0.938\n",
      "Batch 540 >> loss: 1.522 | acc: 0.953\n",
      "Batch 560 >> loss: 1.547 | acc: 0.922\n",
      "Batch 580 >> loss: 1.496 | acc: 0.969\n",
      "Batch 600 >> loss: 1.495 | acc: 0.984\n",
      "Batch 620 >> loss: 1.574 | acc: 0.906\n",
      "Batch 640 >> loss: 1.537 | acc: 0.938\n",
      "Batch 657 >> loss: 1.584 | acc: 0.875\n",
      "Average train loss : 1.550\n",
      "Average train acc  : 0.919\n",
      "\n",
      "Improve! 😄\n",
      "========================================\n",
      "\n",
      "EPOCH 7\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 1.584 | acc: 0.875\n",
      "Batch 40 >> loss: 1.530 | acc: 0.938\n",
      "Batch 60 >> loss: 1.624 | acc: 0.828\n",
      "Batch 80 >> loss: 1.543 | acc: 0.938\n",
      "Batch 100 >> loss: 1.544 | acc: 0.922\n",
      "Batch 120 >> loss: 1.599 | acc: 0.875\n",
      "Batch 140 >> loss: 1.548 | acc: 0.922\n",
      "Batch 160 >> loss: 1.523 | acc: 0.938\n",
      "Batch 180 >> loss: 1.558 | acc: 0.906\n",
      "Batch 200 >> loss: 1.535 | acc: 0.938\n",
      "Batch 220 >> loss: 1.537 | acc: 0.922\n",
      "Batch 240 >> loss: 1.551 | acc: 0.922\n",
      "Batch 260 >> loss: 1.573 | acc: 0.891\n",
      "Batch 280 >> loss: 1.566 | acc: 0.922\n",
      "Batch 300 >> loss: 1.565 | acc: 0.906\n",
      "Batch 320 >> loss: 1.557 | acc: 0.906\n",
      "Batch 340 >> loss: 1.574 | acc: 0.875\n",
      "Batch 360 >> loss: 1.612 | acc: 0.844\n",
      "Batch 380 >> loss: 1.524 | acc: 0.938\n",
      "Batch 400 >> loss: 1.530 | acc: 0.938\n",
      "Batch 420 >> loss: 1.526 | acc: 0.938\n",
      "Batch 440 >> loss: 1.548 | acc: 0.938\n",
      "Batch 460 >> loss: 1.570 | acc: 0.906\n",
      "Batch 480 >> loss: 1.567 | acc: 0.906\n",
      "Batch 500 >> loss: 1.571 | acc: 0.906\n",
      "Batch 520 >> loss: 1.580 | acc: 0.891\n",
      "Batch 540 >> loss: 1.546 | acc: 0.906\n",
      "Batch 560 >> loss: 1.541 | acc: 0.922\n",
      "Batch 580 >> loss: 1.527 | acc: 0.938\n",
      "Batch 600 >> loss: 1.520 | acc: 0.953\n",
      "Batch 620 >> loss: 1.501 | acc: 0.969\n",
      "Batch 640 >> loss: 1.546 | acc: 0.922\n",
      "Batch 657 >> loss: 1.481 | acc: 1.000\n",
      "Average train loss : 1.543\n",
      "Average train acc  : 0.924\n",
      "\n",
      "Improve! 😄\n",
      "========================================\n",
      "\n",
      "EPOCH 8\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 1.608 | acc: 0.875\n",
      "Batch 40 >> loss: 1.550 | acc: 0.922\n",
      "Batch 60 >> loss: 1.508 | acc: 0.969\n",
      "Batch 80 >> loss: 1.577 | acc: 0.875\n",
      "Batch 100 >> loss: 1.540 | acc: 0.922\n",
      "Batch 120 >> loss: 1.532 | acc: 0.953\n",
      "Batch 140 >> loss: 1.541 | acc: 0.922\n",
      "Batch 160 >> loss: 1.529 | acc: 0.938\n",
      "Batch 180 >> loss: 1.507 | acc: 0.969\n",
      "Batch 200 >> loss: 1.541 | acc: 0.938\n",
      "Batch 220 >> loss: 1.585 | acc: 0.891\n",
      "Batch 240 >> loss: 1.529 | acc: 0.922\n",
      "Batch 260 >> loss: 1.567 | acc: 0.906\n",
      "Batch 280 >> loss: 1.575 | acc: 0.906\n",
      "Batch 300 >> loss: 1.591 | acc: 0.875\n",
      "Batch 320 >> loss: 1.551 | acc: 0.906\n",
      "Batch 340 >> loss: 1.535 | acc: 0.938\n",
      "Batch 360 >> loss: 1.537 | acc: 0.922\n",
      "Batch 380 >> loss: 1.542 | acc: 0.922\n",
      "Batch 400 >> loss: 1.547 | acc: 0.906\n",
      "Batch 420 >> loss: 1.503 | acc: 0.969\n",
      "Batch 440 >> loss: 1.548 | acc: 0.922\n",
      "Batch 460 >> loss: 1.560 | acc: 0.906\n",
      "Batch 480 >> loss: 1.531 | acc: 0.938\n",
      "Batch 500 >> loss: 1.604 | acc: 0.828\n",
      "Batch 520 >> loss: 1.527 | acc: 0.938\n",
      "Batch 540 >> loss: 1.513 | acc: 0.969\n",
      "Batch 560 >> loss: 1.542 | acc: 0.922\n",
      "Batch 580 >> loss: 1.537 | acc: 0.922\n",
      "Batch 600 >> loss: 1.514 | acc: 0.953\n",
      "Batch 620 >> loss: 1.537 | acc: 0.938\n",
      "Batch 640 >> loss: 1.559 | acc: 0.891\n",
      "Batch 657 >> loss: 1.727 | acc: 0.750\n",
      "Average train loss : 1.537\n",
      "Average train acc  : 0.931\n",
      "\n",
      "Improve! 😄\n",
      "========================================\n",
      "\n",
      "EPOCH 9\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 1.540 | acc: 0.922\n",
      "Batch 40 >> loss: 1.551 | acc: 0.922\n",
      "Batch 60 >> loss: 1.507 | acc: 0.953\n",
      "Batch 80 >> loss: 1.487 | acc: 0.984\n",
      "Batch 100 >> loss: 1.547 | acc: 0.922\n",
      "Batch 120 >> loss: 1.534 | acc: 0.922\n",
      "Batch 140 >> loss: 1.508 | acc: 0.969\n",
      "Batch 160 >> loss: 1.479 | acc: 0.984\n",
      "Batch 180 >> loss: 1.559 | acc: 0.906\n",
      "Batch 200 >> loss: 1.524 | acc: 0.938\n",
      "Batch 220 >> loss: 1.525 | acc: 0.953\n",
      "Batch 240 >> loss: 1.525 | acc: 0.938\n",
      "Batch 260 >> loss: 1.543 | acc: 0.922\n",
      "Batch 280 >> loss: 1.541 | acc: 0.922\n",
      "Batch 300 >> loss: 1.560 | acc: 0.906\n",
      "Batch 320 >> loss: 1.511 | acc: 0.953\n",
      "Batch 340 >> loss: 1.470 | acc: 0.984\n",
      "Batch 360 >> loss: 1.532 | acc: 0.922\n",
      "Batch 380 >> loss: 1.550 | acc: 0.906\n",
      "Batch 400 >> loss: 1.547 | acc: 0.906\n",
      "Batch 420 >> loss: 1.523 | acc: 0.953\n",
      "Batch 440 >> loss: 1.540 | acc: 0.922\n",
      "Batch 460 >> loss: 1.545 | acc: 0.938\n",
      "Batch 480 >> loss: 1.545 | acc: 0.906\n",
      "Batch 500 >> loss: 1.511 | acc: 0.953\n",
      "Batch 520 >> loss: 1.505 | acc: 0.969\n",
      "Batch 540 >> loss: 1.592 | acc: 0.875\n",
      "Batch 560 >> loss: 1.486 | acc: 0.969\n",
      "Batch 580 >> loss: 1.550 | acc: 0.906\n",
      "Batch 600 >> loss: 1.524 | acc: 0.922\n",
      "Batch 620 >> loss: 1.550 | acc: 0.922\n",
      "Batch 640 >> loss: 1.561 | acc: 0.906\n",
      "Batch 657 >> loss: 1.543 | acc: 0.938\n",
      "Average train loss : 1.533\n",
      "Average train acc  : 0.934\n",
      "\n",
      "Improve! 😄\n",
      "========================================\n",
      "\n",
      "EPOCH 10\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 1.553 | acc: 0.938\n",
      "Batch 40 >> loss: 1.505 | acc: 0.953\n",
      "Batch 60 >> loss: 1.503 | acc: 0.969\n",
      "Batch 80 >> loss: 1.521 | acc: 0.938\n",
      "Batch 100 >> loss: 1.565 | acc: 0.906\n",
      "Batch 120 >> loss: 1.529 | acc: 0.953\n",
      "Batch 140 >> loss: 1.504 | acc: 0.969\n",
      "Batch 160 >> loss: 1.528 | acc: 0.938\n",
      "Batch 180 >> loss: 1.499 | acc: 0.969\n",
      "Batch 200 >> loss: 1.556 | acc: 0.906\n",
      "Batch 220 >> loss: 1.516 | acc: 0.953\n",
      "Batch 240 >> loss: 1.538 | acc: 0.938\n",
      "Batch 260 >> loss: 1.559 | acc: 0.906\n",
      "Batch 280 >> loss: 1.523 | acc: 0.938\n",
      "Batch 300 >> loss: 1.552 | acc: 0.906\n",
      "Batch 320 >> loss: 1.534 | acc: 0.938\n",
      "Batch 340 >> loss: 1.535 | acc: 0.922\n",
      "Batch 360 >> loss: 1.574 | acc: 0.891\n",
      "Batch 380 >> loss: 1.505 | acc: 0.953\n",
      "Batch 400 >> loss: 1.469 | acc: 1.000\n",
      "Batch 420 >> loss: 1.529 | acc: 0.938\n",
      "Batch 440 >> loss: 1.498 | acc: 0.984\n",
      "Batch 460 >> loss: 1.561 | acc: 0.891\n",
      "Batch 480 >> loss: 1.524 | acc: 0.953\n",
      "Batch 500 >> loss: 1.533 | acc: 0.938\n",
      "Batch 520 >> loss: 1.556 | acc: 0.906\n",
      "Batch 540 >> loss: 1.527 | acc: 0.922\n",
      "Batch 560 >> loss: 1.469 | acc: 1.000\n",
      "Batch 580 >> loss: 1.558 | acc: 0.906\n",
      "Batch 600 >> loss: 1.519 | acc: 0.953\n",
      "Batch 620 >> loss: 1.498 | acc: 0.969\n",
      "Batch 640 >> loss: 1.547 | acc: 0.922\n",
      "Batch 657 >> loss: 1.479 | acc: 1.000\n",
      "Average train loss : 1.528\n",
      "Average train acc  : 0.938\n",
      "\n",
      "Improve! 😄\n",
      "========================================\n",
      "\n",
      "EPOCH 11\n",
      "========================================\n",
      "Train\n",
      "Batch 20 >> loss: 1.518 | acc: 0.953\n",
      "Batch 40 >> loss: 1.505 | acc: 0.969\n",
      "Batch 60 >> loss: 1.512 | acc: 0.953\n",
      "Batch 80 >> loss: 1.580 | acc: 0.875\n",
      "Batch 100 >> loss: 1.581 | acc: 0.875\n",
      "Batch 120 >> loss: 1.550 | acc: 0.922\n",
      "Batch 140 >> loss: 1.553 | acc: 0.922\n",
      "Batch 160 >> loss: 1.504 | acc: 0.969\n",
      "Batch 180 >> loss: 1.544 | acc: 0.922\n",
      "Batch 200 >> loss: 1.488 | acc: 0.984\n",
      "Batch 220 >> loss: 1.525 | acc: 0.938\n",
      "Batch 240 >> loss: 1.501 | acc: 0.969\n",
      "Batch 260 >> loss: 1.507 | acc: 0.953\n",
      "Batch 280 >> loss: 1.517 | acc: 0.953\n",
      "Batch 300 >> loss: 1.522 | acc: 0.938\n",
      "Batch 320 >> loss: 1.534 | acc: 0.938\n",
      "Batch 340 >> loss: 1.490 | acc: 0.969\n",
      "Batch 360 >> loss: 1.545 | acc: 0.938\n",
      "Batch 380 >> loss: 1.545 | acc: 0.906\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_losses, train_metrics\n\u001b[0;32m---> 56\u001b[0m train_losses, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mfitting_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mfitting_step\u001b[0;34m(n_epoch, n_penalty, train_dataloader, model, loss_metric, eval_metric_train, eval_metric_test, optimizer, device, show_metric_every)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m train_loss, train_metric \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_metric_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     19\u001b[0m train_metrics\u001b[38;5;241m.\u001b[39mappend(train_metric\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(dataloader, model, loss_metric, eval_metric, optimizer, show_metric_every)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Backward Propagation\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mtensor(losses))\n",
      "File \u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "now = datetime.now(tz=pytz.timezone(\"Asia/Makassar\"))\n",
    "now = now.strftime(\"%m_%d_%Y-%H_%M_%S\")\n",
    "os.makedirs(f\"../callbacks/{now}/epochs\", exist_ok=True)\n",
    "\n",
    "def fitting_step(n_epoch, n_penalty, train_dataloader, model, loss_metric, eval_metric_train, eval_metric_test, optimizer, device, show_metric_every):\n",
    "    epoch = 1\n",
    "    n_current_penalty = 0\n",
    "    train_losses, train_metrics = [], []\n",
    "    # test_losses, test_metrics = [], []\n",
    "    \n",
    "    while True:\n",
    "        print(f\"EPOCH {epoch}\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Train\")\n",
    "        train_loss, train_metric = training_step(train_dataloader, model, loss_metric, eval_metric_train, optimizer, show_metric_every)\n",
    "        train_losses.append(train_loss.to(\"cpu\"))\n",
    "        train_metrics.append(train_metric.to(\"cpu\"))\n",
    "        print(f\"Average train loss : {train_loss:.3f}\")\n",
    "        print(f\"Average train acc  : {train_metric:.3f}\\n\")\n",
    "        \n",
    "        # print(f\"Test\")\n",
    "        # test_loss, test_metric = testing_step(test_dataloader, model, losseval_metric, eval_metric_test, optimizer)\n",
    "        # test_losses.append(test_loss)\n",
    "        # test_metrics.append(test_metric)\n",
    "        # print(f\"Average test loss : {test_loss:.3f}\")\n",
    "        # print(f\"Average test acc  : {test_metric:.3f}\\n\")\n",
    "\n",
    "        if epoch != 1:\n",
    "            if train_losses[-1] > train_losses[-2]:\n",
    "                n_current_penalty += 1\n",
    "                print(f\"Not improve! Number of penalty = {n_current_penalty}/{n_penalty}! 😔\")\n",
    "            elif train_losses[-1] < train_losses[-2] and n_current_penalty != 0:\n",
    "                n_current_penalty -= 1\n",
    "                print(f\"Improve! 😄\")\n",
    "            else:\n",
    "                print(f\"Improve! 😄\")\n",
    "\n",
    "            if n_current_penalty == n_penalty:\n",
    "                print(f\"Number of penalty = {n_current_penalty}/{n_penalty}!, training stopped!\")\n",
    "                break\n",
    "                \n",
    "            # if train_losses[-1] < test_losses[-1]:\n",
    "            #     print(\"Model overfit! Training stopped.\")\n",
    "            #     break\n",
    "        print(\"=\" * 40 + \"\\n\")\n",
    "        torch.save(model.state_dict(), f\"../callbacks/{now}/epochs/{str(epoch).zfill(4)}.pth\")\n",
    "        epoch += 1\n",
    "        eval_metric_train.reset()\n",
    "        eval_metric_test.reset()\n",
    "    torch.save(model.state_dict(), f\"../callbacks/{now}/epochs/{str(epoch).zfill(4)}.pth\")\n",
    "    print(\"Done!\")\n",
    "    return train_losses, train_metrics\n",
    "\n",
    "train_losses, train_metrics = fitting_step(2, 3, train_dataloader, model, loss_metric, eval_metric_train, eval_metric_test, optimizer, device, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974303cf-1595-48c6-a3d8-e761cda4b60c",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb8b6d-e042-431a-9fb4-f0ec5e41e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"../pretrained_models/{now}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5e5c0-68f1-49e5-b21c-3a504cfa224e",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d64ff-6f38-4898-82f7-aeaa1a9f3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, tight_layout=True, figsize=(10, 10))\n",
    "\n",
    "ax[0][0].set_title(\"Train Accuracy\")\n",
    "ax[0][0].plot(train_metrics, label=\"Accuracy\")\n",
    "ax[0][0].set_xlabel(\"Epoch\")\n",
    "ax[0][0].set_ylabel(\"Score\")\n",
    "ax[0][0].grid()\n",
    "ax[0][0].legend()\n",
    "\n",
    "ax[0][1].set_title(\"Train Loss\")\n",
    "ax[0][1].plot(train_losses, label=\"Loss\")\n",
    "ax[0][1].set_xlabel(\"Epoch\")\n",
    "ax[0][1].set_ylabel(\"Score\")\n",
    "ax[0][1].grid()\n",
    "ax[0][1].legend();\n",
    "\n",
    "# ax[1][0].set_title(\"Train and Test Loss\")\n",
    "# ax[1][0].plot(train_losses, label=\"Train\")\n",
    "# ax[1][0].plot(test_losses, label=\"Test\")\n",
    "# ax[1][0].set_xlabel(\"Epoch\")\n",
    "# ax[1][0].set_ylabel(\"Score\")\n",
    "# ax[1][0].grid()\n",
    "# ax[1][0].legend()\n",
    "\n",
    "# ax[1][1].set_title(\"Train and Test Accuracy\")\n",
    "# ax[1][1].plot(train_metrics, label=\"Train\")\n",
    "# ax[1][1].plot(test_metrics, label=\"Test\")\n",
    "# ax[1][1].set_xlabel(\"Epoch\")\n",
    "# ax[1][1].set_ylabel(\"Score\")\n",
    "# ax[1][1].grid()\n",
    "# ax[1][1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befbcf9-7b84-4d2e-8331-83e98376ee31",
   "metadata": {},
   "source": [
    "## X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c36d00-70fd-46c2-998e-81f7e0bda130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(f\"../callbacks/{now}/images\", exist_ok=True)\n",
    "# model.eval()\n",
    "# n_image = 900\n",
    "# idx_start = np.random.randint(0, len(X_test) - 1)\n",
    "# features = X_train[idx_start:idx_start+n_image].to(device)\n",
    "# actual_labels = y_train[idx_start:idx_start+n_image].to(device)\n",
    "# pred_labels = model(features).argmax(1)\n",
    "\n",
    "# plt.figure(figsize=(n_image**0.5, n_image**0.5), tight_layout=True)\n",
    "# for i in range(1, n_image + 1):\n",
    "#     plt.subplot(int(n_image**0.5), int(n_image**0.5), i)\n",
    "#     color_text = \"green\" if pred_labels[i-1] == actual_labels[i-1] else \"red\"\n",
    "#     plt.title(f\"Pred: {pred_labels[i-1]} | Actual {actual_labels[i-1]}\", size=7, color=color_text)\n",
    "#     plt.imshow(features[i-1].reshape((28, 28)).to(\"cpu\"), cmap=plt.cm.gray)\n",
    "#     plt.axis(\"off\")\n",
    "# plt.savefig(f\"../callbacks/{now}/images/X_train_prediction.jpeg\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a1753-d993-422e-99c4-cccacaff83b6",
   "metadata": {},
   "source": [
    "## X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fc974-e8cd-43ac-beaf-04c36244b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(f\"../callbacks/{now}/images\", exist_ok=True)\n",
    "# model.eval()\n",
    "# n_image = 900\n",
    "# idx_start = np.random.randint(0, len(X_test) - 1)\n",
    "# features = X_test[idx_start:idx_start+n_image].to(device)\n",
    "# pred_labels = model(features).argmax(1)\n",
    "\n",
    "# plt.figure(figsize=(n_image**0.5, n_image**0.5), tight_layout=True)\n",
    "# for i in range(1, n_image + 1):\n",
    "#     plt.subplot(int(n_image**0.5), int(n_image**0.5), i)\n",
    "#     plt.title(f\"Pred: {pred_labels[i-1]}\", color=\"black\")\n",
    "#     plt.imshow(features[i-1].reshape((28, 28)).to(\"cpu\"), cmap=plt.cm.gray)\n",
    "#     plt.axis(\"off\")\n",
    "# plt.savefig(f\"../callbacks/{now}/images/X_test_prediction.jpeg\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afad396-69a4-47a1-883a-2f4a387b015a",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e38560-76d6-4ea9-9746-67281fa0c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_labels = model(X_test.to(device)).argmax(1)\n",
    "# submission_df = pd.DataFrame({\n",
    "#     \"ImageId\": np.arange(1, 28001),\n",
    "#     \"Label\": pred_labels.to(\"cpu\")\n",
    "# })\n",
    "\n",
    "# submission_df.to_csv(f\"../submissions/{now}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f8489-2ef2-42da-b1ca-4f1dbf5b59f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f\"../callbacks/{now}/epochs/0074.pth\"))\n",
    "model.eval()\n",
    "pred_labels = model(X_test.to(device)).argmax(1)\n",
    "submission_df = pd.DataFrame({\n",
    "    \"ImageId\": np.arange(1, len(pred_labels) + 1),\n",
    "    \"Label\": pred_labels.to(\"cpu\")\n",
    "})\n",
    "\n",
    "submission_df.to_csv(f\"../submissions/{now}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f7ecd-d1f8-467f-99ec-7ea78fdc7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([params.numel() for params in rnn.parameters() if params.requires_grad == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede03cf-4e60-4ec7-9cae-9ccd6f08edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    if params.requires_grad:\n",
    "        print(f\"{name}: {params.numel()} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a796ec-0d96-40c7-91ab-85e4ad02b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in rnn.named_parameters():\n",
    "    \n",
    "    if params.requires_grad:\n",
    "        print(f\"{type(name)}: {params.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be45e2ad-c86d-4f60-bcbb-0bfa5f0ff717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
